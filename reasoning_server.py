import socket
import json
import traceback
import os
from datetime import datetime

zone_name = "THERMAL ZONE: STORY 6 WEST PERIMETER SPACE"
control_mode = "textgrad"  # Options: "textgrad", "mpc", "llm"

HOST = '127.0.0.1'
PORT = 55555
TIMEOUT_SECONDS = 120  # 2 minutes

retriever = None
generator = None

# Conditionally import phiGPT modules only if needed
if control_mode in ("textgrad", "llm"):
    from phigpt import phiGPTRetriever, phiGPTGenerator

    retriever = phiGPTRetriever(
        ts_db_path_simulation=r".\data\timeseries\ep_simulation",
        ts_db_path_measurement=r".\data\timeseries\multiindex_energyplus_data",
        pdf_db_path=r".\data\text",
        api_key_env="AI_API_KEY",
        api_base_url="https://aiapi-prod.stanford.edu/v1",
        model_name="o3-mini",
        horizon_hours=3,
        target_zone=zone_name
    )

    generator = phiGPTGenerator(
        retriever=retriever,
        api_key_env="AI_API_KEY",
        api_base_url="https://aiapi-prod.stanford.edu/v1",
        model_name="o3-mini"
    )


def handle_request(conn):
    data = conn.recv(8192)
    if not data:
        return
    message = json.loads(data.decode())

    # Shutdown signal
    if message.get("shutdown", False):
        conn.sendall(json.dumps({"status": "server shutting down"}).encode())
        return

    state_buffer = message.get("state_buffer")
    current_time = message.get("current_time", None)

    if state_buffer is None:
        conn.sendall(json.dumps({"error": "No state_buffer provided."}).encode())
        return

    try:
        # üß† Prompt-building path
        if control_mode in ("textgrad", "llm"):
            prompt, ts_know, pdf_sum = retriever.build_cooling_prompt(state_buffer, current_time=current_time)

        if control_mode == "textgrad":
            print("[ReasoningServer] üöÄ Using PhiGPT (TextGrad)...")
            result_raw = generator.optimize_setpoints_with_textgrad(
                prompt_text=prompt,
                ts_knowledge=ts_know,
                pdf_summary=pdf_sum,
                log_path=None,
                zone_name=zone_name,
                max_iters=5,
                current_states=state_buffer
            )

            raw_reason = result_raw.get("reason", "")
            print(f"[DEBUG] Raw reason from result_raw: {repr(raw_reason)}")

            try:
                parsed_reason = json.loads(raw_reason) if isinstance(raw_reason, str) else raw_reason
                reason_text = parsed_reason.get("reason", parsed_reason)
            except Exception:
                reason_text = raw_reason

            result = {
                "optimal_cooling_setpoints": result_raw["optimal_cooling_setpoints"],
                "applied_setpoint": result_raw["applied_setpoint"],
                "reason": reason_text,
                "log_path": result_raw.get("log_path", ""),
                "improved_prompt": result_raw.get("improved_prompt", "")
            }

        elif control_mode == "mpc":
            print("[ReasoningServer] ‚ú® Using MPC optimization...")
            from sim_optimize_setpoints import find_best_setpoint_by_simulation

            best_seq, total_score, energy_score, comfort_score = find_best_setpoint_by_simulation(
                log_path=None,
                zone_name=zone_name,
                w_energy=1.0,
                w_comfort=1.5            )

            reason_str = (
                f"MPC optimization ‚Üí total={total_score:.2f}, energy={energy_score:.2f}, comfort={comfort_score:.2f}"
            )

            result = {
                "optimal_cooling_setpoints": list(best_seq),
                "applied_setpoint": best_seq[0],
                "reason": reason_str
            }

        else:  # LLM single-shot
            print("[ReasoningServer] ‚ú® Using single-shot LLM generation...")
            result_raw = generator.generate_response_from_prompt(prompt, ts_know, pdf_sum)

            result = {
                "optimal_cooling_setpoints": result_raw.get("optimal_cooling_setpoints", [23.0]*4),
                "applied_setpoint": result_raw.get("optimal_cooling_setpoints", [23.0]*4)[0],
                "reason": result_raw.get("reason", "Generated by LLM")
            }

        # üì¢ Debug output
        print("\n[ReasoningServer] üîÑ Cooling Control Decision (Multi-step)")
        print(f"> Setpoints (t0~t3): {result['optimal_cooling_setpoints']}")
        print(f"> Reason: {result['reason']}\n")

        response = {
            "optimal_cooling_setpoints": result["optimal_cooling_setpoints"],
            "applied_setpoint": result["applied_setpoint"],
            "reason": result["reason"]
        }

    except Exception as e:
        print("[ReasoningServer] ‚ö†Ô∏è Error during generation:", e)
        traceback.print_exc()
        response = {"error": str(e)}

    conn.sendall(json.dumps(response).encode())


# Main server loop
with socket.socket() as s:
    s.bind((HOST, PORT))
    s.listen()
    s.settimeout(TIMEOUT_SECONDS)

    print(f"[ReasoningServer] Listening on {HOST}:{PORT} (timeout: {TIMEOUT_SECONDS}s)")

    try:
        while True:
            try:
                conn, _ = s.accept()
                with conn:
                    handle_request(conn)
            except socket.timeout:
                print("[ReasoningServer] ‚è≥ Timeout: No incoming connection. Shutting down.")
                break
    except KeyboardInterrupt:
        print("[ReasoningServer] üßπ Manually interrupted. Shutting down.")

    print("[ReasoningServer] üõë Server shutdown complete.")
